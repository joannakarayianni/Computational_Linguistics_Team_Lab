import pandas as pd
import numpy as np
from transformers import BertTokenizer, TFBertModel
from sklearn.preprocessing import MultiLabelBinarizer
from tensorflow.keras.models import Model
from tensorflow.keras.layers import Input, Dense, Dropout, Layer
from sklearn.metrics import classification_report
import tensorflow as tf

# Load training data
train_df = pd.read_csv('datasets/isear-train.csv', header=None, names=['emotion', 'text'], on_bad_lines='skip')
train_df['text'] = train_df['text'].astype(str)
emotions = ['joy', 'sadness', 'guilt', 'disgust', 'shame', 'fear', 'anger']
train_df = train_df[train_df['emotion'].isin(emotions)]

# Load validation data
val_df = pd.read_csv('datasets/isear-val.csv', header=None, names=['emotion', 'text'], on_bad_lines='skip')
val_df['text'] = val_df['text'].astype(str)
val_df = val_df[val_df['emotion'].isin(emotions)]

# Load BERT tokenizer and model
tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')
bert_model = TFBertModel.from_pretrained('bert-base-uncased')

# Tokenize and encode sequences in the training set
train_encodings = tokenizer(train_df['text'].tolist(), truncation=True, padding=True, max_length=100, return_tensors='tf')
X_train = train_encodings['input_ids']
attention_masks_train = train_encodings['attention_mask']

# Encode the training labels
mlb = MultiLabelBinarizer(classes=emotions)
y_train = mlb.fit_transform(train_df['emotion'].apply(lambda x: [x]))

# Tokenize and encode sequences in the validation set
val_encodings = tokenizer(val_df['text'].tolist(), truncation=True, padding=True, max_length=100, return_tensors='tf')
X_val = val_encodings['input_ids']
attention_masks_val = val_encodings['attention_mask']

# Encode the validation labels
y_val = mlb.transform(val_df['emotion'].apply(lambda x: [x]))

# Define a custom layer to handle the BERT model
class BertLayer(Layer):
    def __init__(self, **kwargs):
        super(BertLayer, self).__init__(**kwargs)
        self.bert = bert_model

    def call(self, inputs):
        input_ids, attention_mask = inputs
        bert_output = self.bert(input_ids, attention_mask=attention_mask)[0]
        return bert_output

# Define the model
input_ids = Input(shape=(100,), dtype=tf.int32, name='input_ids')
attention_mask = Input(shape=(100,), dtype=tf.int32, name='attention_mask')

# Use the custom BertLayer to handle the input tensors
bert_output = BertLayer()([input_ids, attention_mask])
cls_token = bert_output[:, 0, :]
dropout = Dropout(0.3)(cls_token)
output = Dense(units=len(emotions), activation='sigmoid')(dropout)

model = Model(inputs=[input_ids, attention_mask], outputs=output)

# Compile the model
model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])

# Train the model
history = model.fit([X_train, attention_masks_train], y_train, epochs=10, batch_size=32, validation_data=([X_val, attention_masks_val], y_val))

# Evaluate the model on the validation set
loss, accuracy = model.evaluate([X_val, attention_masks_val], y_val, verbose=0)
print(f'Validation Loss: {loss}')
print(f'Validation Accuracy: {accuracy}')

# Predict on the validation set
y_pred = model.predict([X_val, attention_masks_val])
y_pred = (y_pred > 0.5).astype(int)

# Report
print(classification_report(y_val, y_pred, target_names=emotions))

