import pandas as pd
import numpy as np
from transformers import BertTokenizer, TFBertModel
from sklearn.preprocessing import MultiLabelBinarizer
from tensorflow.keras.models import Model
from tensorflow.keras.layers import Input, Dense, Dropout, Layer
from sklearn.metrics import classification_report
import tensorflow as tf
import random

class EmotionClassifier:
    def __init__(self, train_path, val_path, emotions, max_length=100, seed=42):
        self.train_path = train_path
        self.val_path = val_path
        self.emotions = emotions
        self.max_length = max_length
        self.seed = seed

        # Set seeds for reproducibility
        np.random.seed(self.seed)
        tf.random.set_seed(self.seed)
        random.seed(self.seed)

        self.tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')
        self.bert_model = TFBertModel.from_pretrained('bert-base-uncased')

        # Fit the MultiLabelBinarizer with the emotions list
        self.mlb = MultiLabelBinarizer(classes=self.emotions)
        self.mlb.fit([[emotion] for emotion in self.emotions])

        self.train_df, self.val_df = self.load_data()
        self.X_train, self.attention_masks_train, self.y_train = self.encode_data(self.train_df)
        self.X_val, self.attention_masks_val, self.y_val = self.encode_data(self.val_df, training=False)

        self.model = self.build_model()

    def load_data(self):
        # Load training data
        train_df = pd.read_csv(self.train_path, header=None, names=['emotion', 'text'], on_bad_lines='skip')
        train_df['text'] = train_df['text'].astype(str)
        train_df = train_df[train_df['emotion'].isin(self.emotions)]

        # Load validation data
        val_df = pd.read_csv(self.val_path, header=None, names=['emotion', 'text'], on_bad_lines='skip')
        val_df['text'] = val_df['text'].astype(str)
        val_df = val_df[val_df['emotion'].isin(self.emotions)]

        return train_df, val_df

    def encode_data(self, df, training=True):
        encodings = self.tokenizer(df['text'].tolist(), truncation=True, padding=True, max_length=self.max_length, return_tensors='tf')
        input_ids = encodings['input_ids']
        attention_masks = encodings['attention_mask']

        if training:
            labels = self.mlb.transform(df['emotion'].apply(lambda x: [x]))
        else:
            labels = self.mlb.transform(df['emotion'].apply(lambda x: [x]))

        return input_ids, attention_masks, labels

    class BertLayer(Layer):
        def __init__(self, bert_model, **kwargs):
            super().__init__(**kwargs)
            self.bert = bert_model

        def call(self, inputs):
            input_ids, attention_mask = inputs
            bert_output = self.bert(input_ids, attention_mask=attention_mask)[0]
            return bert_output

    def build_model(self):
        input_ids = Input(shape=(self.max_length,), dtype=tf.int32, name='input_ids')
        attention_mask = Input(shape=(self.max_length,), dtype=tf.int32, name='attention_mask')

        bert_output = self.BertLayer(self.bert_model)([input_ids, attention_mask])
        cls_token = bert_output[:, 0, :]
        dropout = Dropout(0.3)(cls_token)
        output = Dense(units=len(self.emotions), activation='sigmoid')(dropout)

        model = Model(inputs=[input_ids, attention_mask], outputs=output)
        model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])

        return model

    def train(self, epochs=10, batch_size=32):
        history = self.model.fit(
            [self.X_train, self.attention_masks_train], self.y_train,
            epochs=epochs, batch_size=batch_size,
            validation_data=([self.X_val, self.attention_masks_val], self.y_val)
        )
        return history

    def evaluate(self):
        loss, accuracy = self.model.evaluate([self.X_val, self.attention_masks_val], self.y_val, verbose=0)
        print(f'Validation Loss: {loss}')
        print(f'Validation Accuracy: {accuracy}')
        return loss, accuracy

    def predict(self):
        y_pred = self.model.predict([self.X_val, self.attention_masks_val])
        y_pred = (y_pred > 0.5).astype(int)
        print(classification_report(self.y_val, y_pred, target_names=self.emotions))
        return y_pred


# Example usage
train_path = 'datasets/isear-train.csv'
val_path = 'datasets/isear-val.csv'
emotions = ['joy', 'sadness', 'guilt', 'disgust', 'shame', 'fear', 'anger']

classifier = EmotionClassifier(train_path, val_path, emotions)
classifier.train(epochs=10, batch_size=32)
classifier.evaluate()
classifier.predict()
